{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Wine Classification"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import sklearn\n","import matplotlib.pyplot as plt\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Load dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 178 entries, 0 to 177\nData columns (total 14 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   Class                         178 non-null    int64  \n 1   Alcohol                       178 non-null    float64\n 2   Malic acid                    178 non-null    float64\n 3   Ash                           178 non-null    float64\n 4   Alcalinity of ash             178 non-null    float64\n 5   Magnesium                     178 non-null    int64  \n 6   Total phenols                 178 non-null    float64\n 7   Flavanoids                    178 non-null    float64\n 8   Nonflavanoid phenols          178 non-null    float64\n 9   Proanthocyanins               178 non-null    float64\n 10  Color intensity               178 non-null    float64\n 11  Hue                           178 non-null    float64\n 12  OD280/OD315 of diluted wines  178 non-null    float64\n 13  Proline                       178 non-null    int64  \ndtypes: float64(11), int64(3)\nmemory usage: 19.6 KB\n"}],"source":["url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n","names = (\n","    \"Class\",\n","    \"Alcohol\",\n","    \"Malic acid\",\n","    \"Ash\",\n","    \"Alcalinity of ash\",\n","    \"Magnesium\",\n","    \"Total phenols\",\n","    \"Flavanoids\",\n","    \"Nonflavanoid phenols\",\n","    \"Proanthocyanins\",\n","    \"Color intensity\",\n","    \"Hue\",\n","    \"OD280/OD315 of diluted wines\",\n","    \"Proline\"\n","    )\n","dataset = pd.read_csv(url, names=names, delimiter=',')\n","dataset.info()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Split feature and target"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"(178, 14)\n(178, 13)\n(178,)\n"}],"source":["x = dataset.drop([\"Class\"], axis=1) # axis=1 berarti kolom, axis-0 berarti row\n","y = dataset[\"Class\"]\n","\n","print(dataset.shape)\n","print(x.shape)\n","print(y.shape)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## One-hot-encode on target"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"1  2  3\n98   0  1  0\n19   1  0  0\n55   1  0  0\n85   0  1  0\n79   0  1  0\n29   1  0  0\n138  0  0  1\n163  0  0  1\n31   1  0  0\n77   0  1  0\n(178, 3)\n"}],"source":["y = pd.get_dummies(y)\n","print(y.sample(10))\n","print(y.shape)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Normalization"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[[0.84210526 0.1916996  0.57219251 ... 0.45528455 0.97069597 0.56134094]\n [0.57105263 0.2055336  0.4171123  ... 0.46341463 0.78021978 0.55064194]\n [0.56052632 0.3201581  0.70053476 ... 0.44715447 0.6959707  0.64693295]\n ...\n [0.58947368 0.69960474 0.48128342 ... 0.08943089 0.10622711 0.39728959]\n [0.56315789 0.36561265 0.54010695 ... 0.09756098 0.12820513 0.40085592]\n [0.81578947 0.66403162 0.73796791 ... 0.10569106 0.12087912 0.20114123]]\n"}],"source":["from sklearn.preprocessing import MinMaxScaler\n","x = MinMaxScaler().fit_transform(x)\n","print(x)"]},{"cell_type":"markdown","metadata":{},"source":[" ## Split train and test"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"x_train : (124, 13)\nx_test : (54, 13)\ny_train : (124, 3)\ny_test : (54, 3)\n"}],"source":["from sklearn.model_selection import train_test_split\n","\n","y = y.to_numpy()\n","\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n","\n","print(f\"x_train : {x_train.shape}\")\n","print(f\"x_test : {x_test.shape}\")\n","print(f\"y_train : {y_train.shape}\")\n","print(f\"y_test : {y_test.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":[" ## See correlation of each feature"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.000000</td>\n      <td>0.094397</td>\n      <td>0.211545</td>\n      <td>-0.310235</td>\n      <td>0.270798</td>\n      <td>0.289101</td>\n      <td>0.236815</td>\n      <td>-0.155929</td>\n      <td>0.136698</td>\n      <td>0.546364</td>\n      <td>-0.071747</td>\n      <td>0.072343</td>\n      <td>0.643720</td>\n      <td>0.647232</td>\n      <td>-0.726383</td>\n      <td>0.114941</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.094397</td>\n      <td>1.000000</td>\n      <td>0.164045</td>\n      <td>0.288500</td>\n      <td>-0.054575</td>\n      <td>-0.335167</td>\n      <td>-0.411007</td>\n      <td>0.292977</td>\n      <td>-0.220746</td>\n      <td>0.248985</td>\n      <td>-0.561296</td>\n      <td>-0.368710</td>\n      <td>-0.192011</td>\n      <td>-0.205847</td>\n      <td>-0.295175</td>\n      <td>0.544042</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.211545</td>\n      <td>0.164045</td>\n      <td>1.000000</td>\n      <td>0.443367</td>\n      <td>0.286587</td>\n      <td>0.128980</td>\n      <td>0.115077</td>\n      <td>0.186230</td>\n      <td>0.009652</td>\n      <td>0.258887</td>\n      <td>-0.074667</td>\n      <td>0.003911</td>\n      <td>0.223626</td>\n      <td>0.229268</td>\n      <td>-0.362457</td>\n      <td>0.156738</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.310235</td>\n      <td>0.288500</td>\n      <td>0.443367</td>\n      <td>1.000000</td>\n      <td>-0.083333</td>\n      <td>-0.321113</td>\n      <td>-0.351370</td>\n      <td>0.361922</td>\n      <td>-0.197327</td>\n      <td>0.018732</td>\n      <td>-0.273955</td>\n      <td>-0.276769</td>\n      <td>-0.440597</td>\n      <td>-0.519646</td>\n      <td>0.181764</td>\n      <td>0.350650</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.270798</td>\n      <td>-0.054575</td>\n      <td>0.286587</td>\n      <td>-0.083333</td>\n      <td>1.000000</td>\n      <td>0.214401</td>\n      <td>0.195784</td>\n      <td>-0.256294</td>\n      <td>0.236441</td>\n      <td>0.199950</td>\n      <td>0.055398</td>\n      <td>0.066004</td>\n      <td>0.393351</td>\n      <td>0.326171</td>\n      <td>-0.296972</td>\n      <td>-0.018306</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.289101</td>\n      <td>-0.335167</td>\n      <td>0.128980</td>\n      <td>-0.321113</td>\n      <td>0.214401</td>\n      <td>1.000000</td>\n      <td>0.864564</td>\n      <td>-0.449935</td>\n      <td>0.612413</td>\n      <td>-0.055136</td>\n      <td>0.433681</td>\n      <td>0.699949</td>\n      <td>0.498115</td>\n      <td>0.614960</td>\n      <td>-0.047301</td>\n      <td>-0.600119</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.236815</td>\n      <td>-0.411007</td>\n      <td>0.115077</td>\n      <td>-0.351370</td>\n      <td>0.195784</td>\n      <td>0.864564</td>\n      <td>1.000000</td>\n      <td>-0.537900</td>\n      <td>0.652692</td>\n      <td>-0.172379</td>\n      <td>0.543479</td>\n      <td>0.787194</td>\n      <td>0.494193</td>\n      <td>0.673770</td>\n      <td>0.042179</td>\n      <td>-0.761232</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.155929</td>\n      <td>0.292977</td>\n      <td>0.186230</td>\n      <td>0.361922</td>\n      <td>-0.256294</td>\n      <td>-0.449935</td>\n      <td>-0.537900</td>\n      <td>1.000000</td>\n      <td>-0.365845</td>\n      <td>0.139057</td>\n      <td>-0.262640</td>\n      <td>-0.503270</td>\n      <td>-0.311385</td>\n      <td>-0.407680</td>\n      <td>0.011868</td>\n      <td>0.419347</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.136698</td>\n      <td>-0.220746</td>\n      <td>0.009652</td>\n      <td>-0.197327</td>\n      <td>0.236441</td>\n      <td>0.612413</td>\n      <td>0.652692</td>\n      <td>-0.365845</td>\n      <td>1.000000</td>\n      <td>-0.025250</td>\n      <td>0.295544</td>\n      <td>0.519067</td>\n      <td>0.330417</td>\n      <td>0.380500</td>\n      <td>0.056208</td>\n      <td>-0.465629</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.546364</td>\n      <td>0.248985</td>\n      <td>0.258887</td>\n      <td>0.018732</td>\n      <td>0.199950</td>\n      <td>-0.055136</td>\n      <td>-0.172379</td>\n      <td>0.139057</td>\n      <td>-0.025250</td>\n      <td>1.000000</td>\n      <td>-0.521813</td>\n      <td>-0.428815</td>\n      <td>0.316100</td>\n      <td>0.143221</td>\n      <td>-0.694679</td>\n      <td>0.614582</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-0.071747</td>\n      <td>-0.561296</td>\n      <td>-0.074667</td>\n      <td>-0.273955</td>\n      <td>0.055398</td>\n      <td>0.433681</td>\n      <td>0.543479</td>\n      <td>-0.262640</td>\n      <td>0.295544</td>\n      <td>-0.521813</td>\n      <td>1.000000</td>\n      <td>0.565468</td>\n      <td>0.236183</td>\n      <td>0.323088</td>\n      <td>0.353213</td>\n      <td>-0.732443</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.072343</td>\n      <td>-0.368710</td>\n      <td>0.003911</td>\n      <td>-0.276769</td>\n      <td>0.066004</td>\n      <td>0.699949</td>\n      <td>0.787194</td>\n      <td>-0.503270</td>\n      <td>0.519067</td>\n      <td>-0.428815</td>\n      <td>0.565468</td>\n      <td>1.000000</td>\n      <td>0.312761</td>\n      <td>0.543131</td>\n      <td>0.199813</td>\n      <td>-0.796590</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.643720</td>\n      <td>-0.192011</td>\n      <td>0.223626</td>\n      <td>-0.440597</td>\n      <td>0.393351</td>\n      <td>0.498115</td>\n      <td>0.494193</td>\n      <td>-0.311385</td>\n      <td>0.330417</td>\n      <td>0.316100</td>\n      <td>0.236183</td>\n      <td>0.312761</td>\n      <td>1.000000</td>\n      <td>0.827000</td>\n      <td>-0.589850</td>\n      <td>-0.226394</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.647232</td>\n      <td>-0.205847</td>\n      <td>0.229268</td>\n      <td>-0.519646</td>\n      <td>0.326171</td>\n      <td>0.614960</td>\n      <td>0.673770</td>\n      <td>-0.407680</td>\n      <td>0.380500</td>\n      <td>0.143221</td>\n      <td>0.323088</td>\n      <td>0.543131</td>\n      <td>0.827000</td>\n      <td>1.000000</td>\n      <td>-0.573574</td>\n      <td>-0.427860</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.726383</td>\n      <td>-0.295175</td>\n      <td>-0.362457</td>\n      <td>0.181764</td>\n      <td>-0.296972</td>\n      <td>-0.047301</td>\n      <td>0.042179</td>\n      <td>0.011868</td>\n      <td>0.056208</td>\n      <td>-0.694679</td>\n      <td>0.353213</td>\n      <td>0.199813</td>\n      <td>-0.589850</td>\n      <td>-0.573574</td>\n      <td>1.000000</td>\n      <td>-0.494978</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.114941</td>\n      <td>0.544042</td>\n      <td>0.156738</td>\n      <td>0.350650</td>\n      <td>-0.018306</td>\n      <td>-0.600119</td>\n      <td>-0.761232</td>\n      <td>0.419347</td>\n      <td>-0.465629</td>\n      <td>0.614582</td>\n      <td>-0.732443</td>\n      <td>-0.796590</td>\n      <td>-0.226394</td>\n      <td>-0.427860</td>\n      <td>-0.494978</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          0         1         2         3         4         5         6   \\\n0   1.000000  0.094397  0.211545 -0.310235  0.270798  0.289101  0.236815   \n1   0.094397  1.000000  0.164045  0.288500 -0.054575 -0.335167 -0.411007   \n2   0.211545  0.164045  1.000000  0.443367  0.286587  0.128980  0.115077   \n3  -0.310235  0.288500  0.443367  1.000000 -0.083333 -0.321113 -0.351370   \n4   0.270798 -0.054575  0.286587 -0.083333  1.000000  0.214401  0.195784   \n5   0.289101 -0.335167  0.128980 -0.321113  0.214401  1.000000  0.864564   \n6   0.236815 -0.411007  0.115077 -0.351370  0.195784  0.864564  1.000000   \n7  -0.155929  0.292977  0.186230  0.361922 -0.256294 -0.449935 -0.537900   \n8   0.136698 -0.220746  0.009652 -0.197327  0.236441  0.612413  0.652692   \n9   0.546364  0.248985  0.258887  0.018732  0.199950 -0.055136 -0.172379   \n10 -0.071747 -0.561296 -0.074667 -0.273955  0.055398  0.433681  0.543479   \n11  0.072343 -0.368710  0.003911 -0.276769  0.066004  0.699949  0.787194   \n12  0.643720 -0.192011  0.223626 -0.440597  0.393351  0.498115  0.494193   \n0   0.647232 -0.205847  0.229268 -0.519646  0.326171  0.614960  0.673770   \n1  -0.726383 -0.295175 -0.362457  0.181764 -0.296972 -0.047301  0.042179   \n2   0.114941  0.544042  0.156738  0.350650 -0.018306 -0.600119 -0.761232   \n\n          7         8         9         10        11        12        0   \\\n0  -0.155929  0.136698  0.546364 -0.071747  0.072343  0.643720  0.647232   \n1   0.292977 -0.220746  0.248985 -0.561296 -0.368710 -0.192011 -0.205847   \n2   0.186230  0.009652  0.258887 -0.074667  0.003911  0.223626  0.229268   \n3   0.361922 -0.197327  0.018732 -0.273955 -0.276769 -0.440597 -0.519646   \n4  -0.256294  0.236441  0.199950  0.055398  0.066004  0.393351  0.326171   \n5  -0.449935  0.612413 -0.055136  0.433681  0.699949  0.498115  0.614960   \n6  -0.537900  0.652692 -0.172379  0.543479  0.787194  0.494193  0.673770   \n7   1.000000 -0.365845  0.139057 -0.262640 -0.503270 -0.311385 -0.407680   \n8  -0.365845  1.000000 -0.025250  0.295544  0.519067  0.330417  0.380500   \n9   0.139057 -0.025250  1.000000 -0.521813 -0.428815  0.316100  0.143221   \n10 -0.262640  0.295544 -0.521813  1.000000  0.565468  0.236183  0.323088   \n11 -0.503270  0.519067 -0.428815  0.565468  1.000000  0.312761  0.543131   \n12 -0.311385  0.330417  0.316100  0.236183  0.312761  1.000000  0.827000   \n0  -0.407680  0.380500  0.143221  0.323088  0.543131  0.827000  1.000000   \n1   0.011868  0.056208 -0.694679  0.353213  0.199813 -0.589850 -0.573574   \n2   0.419347 -0.465629  0.614582 -0.732443 -0.796590 -0.226394 -0.427860   \n\n          1         2   \n0  -0.726383  0.114941  \n1  -0.295175  0.544042  \n2  -0.362457  0.156738  \n3   0.181764  0.350650  \n4  -0.296972 -0.018306  \n5  -0.047301 -0.600119  \n6   0.042179 -0.761232  \n7   0.011868  0.419347  \n8   0.056208 -0.465629  \n9  -0.694679  0.614582  \n10  0.353213 -0.732443  \n11  0.199813 -0.796590  \n12 -0.589850 -0.226394  \n0  -0.573574 -0.427860  \n1   1.000000 -0.494978  \n2  -0.494978  1.000000  "},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["corr = pd.concat([pd.DataFrame(x), pd.DataFrame(y)], axis=1, sort=False)\n","corr.corr(method='pearson')\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Train"]},{"cell_type":"code","execution_count":8,"metadata":{"tags":["outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend"]},"outputs":[{"name":"stdout","output_type":"stream","text":", loss = 0.22454478\nIteration 1253, loss = 0.22430787\nIteration 1254, loss = 0.22407142\nIteration 1255, loss = 0.22383546\nIteration 1256, loss = 0.22359991\nIteration 1257, loss = 0.22336485\nIteration 1258, loss = 0.22313021\nIteration 1259, loss = 0.22289605\nIteration 1260, loss = 0.22266233\nIteration 1261, loss = 0.22242907\nIteration 1262, loss = 0.22219627\nIteration 1263, loss = 0.22196388\nIteration 1264, loss = 0.22173199\nIteration 1265, loss = 0.22150049\nIteration 1266, loss = 0.22126943\nIteration 1267, loss = 0.22103885\nIteration 1268, loss = 0.22080866\nIteration 1269, loss = 0.22057892\nIteration 1270, loss = 0.22034962\nIteration 1271, loss = 0.22012077\nIteration 1272, loss = 0.21989234\nIteration 1273, loss = 0.21966437\nIteration 1274, loss = 0.21943681\nIteration 1275, loss = 0.21920965\nIteration 1276, loss = 0.21898294\nIteration 1277, loss = 0.21875663\nIteration 1278, loss = 0.21853079\nIteration 1279, loss = 0.21830534\nIteration 1280, loss = 0.21808031\nIteration 1281, loss = 0.21785568\nIteration 1282, loss = 0.21763143\nIteration 1283, loss = 0.21740762\nIteration 1284, loss = 0.21718418\nIteration 1285, loss = 0.21696118\nIteration 1286, loss = 0.21673859\nIteration 1287, loss = 0.21651639\nIteration 1288, loss = 0.21629462\nIteration 1289, loss = 0.21607323\nIteration 1290, loss = 0.21585225\nIteration 1291, loss = 0.21563175\nIteration 1292, loss = 0.21541169\nIteration 1293, loss = 0.21519204\nIteration 1294, loss = 0.21497281\nIteration 1295, loss = 0.21475403\nIteration 1296, loss = 0.21453565\nIteration 1297, loss = 0.21431764\nIteration 1298, loss = 0.21410008\nIteration 1299, loss = 0.21388287\nIteration 1300, loss = 0.21366617\nIteration 1301, loss = 0.21344987\nIteration 1302, loss = 0.21323397\nIteration 1303, loss = 0.21301846\nIteration 1304, loss = 0.21280335\nIteration 1305, loss = 0.21258864\nIteration 1306, loss = 0.21237429\nIteration 1307, loss = 0.21216033\nIteration 1308, loss = 0.21194674\nIteration 1309, loss = 0.21173353\nIteration 1310, loss = 0.21152073\nIteration 1311, loss = 0.21130828\nIteration 1312, loss = 0.21109622\nIteration 1313, loss = 0.21088454\nIteration 1314, loss = 0.21067324\nIteration 1315, loss = 0.21046238\nIteration 1316, loss = 0.21025190\nIteration 1317, loss = 0.21004180\nIteration 1318, loss = 0.20983206\nIteration 1319, loss = 0.20962272\nIteration 1320, loss = 0.20941372\nIteration 1321, loss = 0.20920510\nIteration 1322, loss = 0.20899687\nIteration 1323, loss = 0.20878898\nIteration 1324, loss = 0.20858148\nIteration 1325, loss = 0.20837435\nIteration 1326, loss = 0.20816759\nIteration 1327, loss = 0.20796122\nIteration 1328, loss = 0.20775522\nIteration 1329, loss = 0.20754958\nIteration 1330, loss = 0.20734430\nIteration 1331, loss = 0.20713940\nIteration 1332, loss = 0.20693486\nIteration 1333, loss = 0.20673069\nIteration 1334, loss = 0.20652692\nIteration 1335, loss = 0.20632349\nIteration 1336, loss = 0.20612043\nIteration 1337, loss = 0.20591771\nIteration 1338, loss = 0.20571536\nIteration 1339, loss = 0.20551337\nIteration 1340, loss = 0.20531171\nIteration 1341, loss = 0.20511045\nIteration 1342, loss = 0.20490952\nIteration 1343, loss = 0.20470894\nIteration 1344, loss = 0.20450871\nIteration 1345, loss = 0.20430888\nIteration 1346, loss = 0.20410935\nIteration 1347, loss = 0.20391017\nIteration 1348, loss = 0.20371133\nIteration 1349, loss = 0.20351283\nIteration 1350, loss = 0.20331466\nIteration 1351, loss = 0.20311683\nIteration 1352, loss = 0.20291936\nIteration 1353, loss = 0.20272222\nIteration 1354, loss = 0.20252545\nIteration 1355, loss = 0.20232904\nIteration 1356, loss = 0.20213295\nIteration 1357, loss = 0.20193723\nIteration 1358, loss = 0.20174181\nIteration 1359, loss = 0.20154676\nIteration 1360, loss = 0.20135202\nIteration 1361, loss = 0.20115763\nIteration 1362, loss = 0.20096356\nIteration 1363, loss = 0.20076982\nIteration 1364, loss = 0.20057646\nIteration 1365, loss = 0.20038339\nIteration 1366, loss = 0.20019075\nIteration 1367, loss = 0.19999835\nIteration 1368, loss = 0.19980631\nIteration 1369, loss = 0.19961463\nIteration 1370, loss = 0.19942324\nIteration 1371, loss = 0.19923219\nIteration 1372, loss = 0.19904146\nIteration 1373, loss = 0.19885112\nIteration 1374, loss = 0.19866102\nIteration 1375, loss = 0.19847129\nIteration 1376, loss = 0.19828192\nIteration 1377, loss = 0.19809283\nIteration 1378, loss = 0.19790416\nIteration 1379, loss = 0.19771587\nIteration 1380, loss = 0.19752788\nIteration 1381, loss = 0.19734024\nIteration 1382, loss = 0.19715287\nIteration 1383, loss = 0.19696582\nIteration 1384, loss = 0.19677907\nIteration 1385, loss = 0.19659270\nIteration 1386, loss = 0.19640657\nIteration 1387, loss = 0.19622078\nIteration 1388, loss = 0.19603532\nIteration 1389, loss = 0.19585012\nIteration 1390, loss = 0.19566532\nIteration 1391, loss = 0.19548078\nIteration 1392, loss = 0.19529661\nIteration 1393, loss = 0.19511277\nIteration 1394, loss = 0.19492919\nIteration 1395, loss = 0.19474596\nIteration 1396, loss = 0.19456315\nIteration 1397, loss = 0.19438053\nIteration 1398, loss = 0.19419829\nIteration 1399, loss = 0.19401630\nIteration 1400, loss = 0.19383464\nIteration 1401, loss = 0.19365331\nIteration 1402, loss = 0.19347229\nIteration 1403, loss = 0.19329157\nIteration 1404, loss = 0.19311115\nIteration 1405, loss = 0.19293104\nIteration 1406, loss = 0.19275120\nIteration 1407, loss = 0.19257166\nIteration 1408, loss = 0.19239251\nIteration 1409, loss = 0.19221353\nIteration 1410, loss = 0.19203493\nIteration 1411, loss = 0.19185658\nIteration 1412, loss = 0.19167859\nIteration 1413, loss = 0.19150086\nIteration 1414, loss = 0.19132341\nIteration 1415, loss = 0.19114627\nIteration 1416, loss = 0.19096940\nIteration 1417, loss = 0.19079281\nIteration 1418, loss = 0.19061654\nIteration 1419, loss = 0.19044058\nIteration 1420, loss = 0.19026485\nIteration 1421, loss = 0.19008943\nIteration 1422, loss = 0.18991429\nIteration 1423, loss = 0.18973944\nIteration 1424, loss = 0.18956491\nIteration 1425, loss = 0.18939062\nIteration 1426, loss = 0.18921659\nIteration 1427, loss = 0.18904288\nIteration 1428, loss = 0.18886941\nIteration 1429, loss = 0.18869625\nIteration 1430, loss = 0.18852337\nIteration 1431, loss = 0.18835075\nIteration 1432, loss = 0.18817840\nIteration 1433, loss = 0.18800639\nIteration 1434, loss = 0.18783460\nIteration 1435, loss = 0.18766320\nIteration 1436, loss = 0.18749194\nIteration 1437, loss = 0.18732104\nIteration 1438, loss = 0.18715037\nIteration 1439, loss = 0.18697999\nIteration 1440, loss = 0.18680990\nIteration 1441, loss = 0.18664006\nIteration 1442, loss = 0.18647050\nIteration 1443, loss = 0.18630119\nIteration 1444, loss = 0.18613214\nIteration 1445, loss = 0.18596340\nIteration 1446, loss = 0.18579489\nIteration 1447, loss = 0.18562668\nIteration 1448, loss = 0.18545871\nIteration 1449, loss = 0.18529103\nIteration 1450, loss = 0.18512366\nIteration 1451, loss = 0.18495648\nIteration 1452, loss = 0.18478961\nIteration 1453, loss = 0.18462298\nIteration 1454, loss = 0.18445664\nIteration 1455, loss = 0.18429056\nIteration 1456, loss = 0.18412471\nIteration 1457, loss = 0.18395913\nIteration 1458, loss = 0.18379376\nIteration 1459, loss = 0.18362866\nIteration 1460, loss = 0.18346387\nIteration 1461, loss = 0.18329927\nIteration 1462, loss = 0.18313498\nIteration 1463, loss = 0.18297090\nIteration 1464, loss = 0.18280713\nIteration 1465, loss = 0.18264362\nIteration 1466, loss = 0.18248032\nIteration 1467, loss = 0.18231731\nIteration 1468, loss = 0.18215452\nIteration 1469, loss = 0.18199202\nIteration 1470, loss = 0.18182987\nIteration 1471, loss = 0.18166792\nIteration 1472, loss = 0.18150631\nIteration 1473, loss = 0.18134496\nIteration 1474, loss = 0.18118386\nIteration 1475, loss = 0.18102300\nIteration 1476, loss = 0.18086238\nIteration 1477, loss = 0.18070203\nIteration 1478, loss = 0.18054190\nIteration 1479, loss = 0.18038209\nIteration 1480, loss = 0.18022246\nIteration 1481, loss = 0.18006312\nIteration 1482, loss = 0.17990402\nIteration 1483, loss = 0.17974522\nIteration 1484, loss = 0.17958661\nIteration 1485, loss = 0.17942826\nIteration 1486, loss = 0.17927021\nIteration 1487, loss = 0.17911234\nIteration 1488, loss = 0.17895477\nIteration 1489, loss = 0.17879741\nIteration 1490, loss = 0.17864029\nIteration 1491, loss = 0.17848342\nIteration 1492, loss = 0.17832677\nIteration 1493, loss = 0.17817048\nIteration 1494, loss = 0.17801427\nIteration 1495, loss = 0.17785840\nIteration 1496, loss = 0.17770279\nIteration 1497, loss = 0.17754740\nIteration 1498, loss = 0.17739221\nIteration 1499, loss = 0.17723714\nIteration 1500, loss = 0.17708230\nIteration 1501, loss = 0.17692771\nIteration 1502, loss = 0.17677331\nIteration 1503, loss = 0.17661920\nIteration 1504, loss = 0.17646527\nIteration 1505, loss = 0.17631163\nIteration 1506, loss = 0.17615814\nIteration 1507, loss = 0.17600491\nIteration 1508, loss = 0.17585187\nIteration 1509, loss = 0.17569908\nIteration 1510, loss = 0.17554652\nIteration 1511, loss = 0.17539415\nIteration 1512, loss = 0.17524201\nIteration 1513, loss = 0.17509009\nIteration 1514, loss = 0.17493842\nIteration 1515, loss = 0.17478695\nIteration 1516, loss = 0.17463568\nIteration 1517, loss = 0.17448466\nIteration 1518, loss = 0.17433388\nIteration 1519, loss = 0.17418330\nIteration 1520, loss = 0.17403295\nIteration 1521, loss = 0.17388283\nIteration 1522, loss = 0.17373298\nIteration 1523, loss = 0.17358332\nIteration 1524, loss = 0.17343391\nIteration 1525, loss = 0.17328469\nIteration 1526, loss = 0.17313576\nIteration 1527, loss = 0.17298695\nIteration 1528, loss = 0.17283842\nIteration 1529, loss = 0.17269020\nIteration 1530, loss = 0.17254210\nIteration 1531, loss = 0.17239424\nIteration 1532, loss = 0.17224661\nIteration 1533, loss = 0.17209918\nIteration 1534, loss = 0.17195200\nIteration 1535, loss = 0.17180499\nIteration 1536, loss = 0.17165820\nIteration 1537, loss = 0.17151162\nIteration 1538, loss = 0.17136529\nIteration 1539, loss = 0.17121916\nIteration 1540, loss = 0.17107320\nIteration 1541, loss = 0.17092750\nIteration 1542, loss = 0.17078204\nIteration 1543, loss = 0.17063672\nIteration 1544, loss = 0.17049169\nIteration 1545, loss = 0.17034687\nIteration 1546, loss = 0.17020227\nIteration 1547, loss = 0.17005791\nIteration 1548, loss = 0.16991369\nIteration 1549, loss = 0.16976980\nIteration 1550, loss = 0.16962602\nIteration 1551, loss = 0.16948253\nIteration 1552, loss = 0.16933913\nIteration 1553, loss = 0.16919606\nIteration 1554, loss = 0.16905311\nIteration 1555, loss = 0.16891042\nIteration 1556, loss = 0.16876795\nIteration 1557, loss = 0.16862562\nIteration 1558, loss = 0.16848355\nIteration 1559, loss = 0.16834168\nIteration 1560, loss = 0.16820009\nIteration 1561, loss = 0.16805862\nIteration 1562, loss = 0.16791738\nIteration 1563, loss = 0.16777640\nIteration 1564, loss = 0.16763566\nIteration 1565, loss = 0.16749508\nIteration 1566, loss = 0.16735472\nIteration 1567, loss = 0.16721457\nIteration 1568, loss = 0.16707468\nIteration 1569, loss = 0.16693490\nIteration 1570, loss = 0.16679536\nIteration 1571, loss = 0.16665609\nIteration 1572, loss = 0.16651696\nIteration 1573, loss = 0.16637803\nIteration 1574, loss = 0.16623928\nIteration 1575, loss = 0.16610079\nIteration 1576, loss = 0.16596246\nIteration 1577, loss = 0.16582434\nIteration 1578, loss = 0.16568641\nIteration 1579, loss = 0.16554867\nIteration 1580, loss = 0.16541118\nIteration 1581, loss = 0.16527382\nIteration 1582, loss = 0.16513668\nIteration 1583, loss = 0.16499970\nIteration 1584, loss = 0.16486298\nIteration 1585, loss = 0.16472641\nIteration 1586, loss = 0.16459002\nIteration 1587, loss = 0.16445389\nIteration 1588, loss = 0.16431786\nIteration 1589, loss = 0.16418208\nIteration 1590, loss = 0.16404647\nIteration 1591, loss = 0.16391105\nIteration 1592, loss = 0.16377587\nIteration 1593, loss = 0.16364083\nIteration 1594, loss = 0.16350599\nIteration 1595, loss = 0.16337133\nIteration 1596, loss = 0.16323691\nIteration 1597, loss = 0.16310264\nIteration 1598, loss = 0.16296851\nIteration 1599, loss = 0.16283464\nIteration 1600, loss = 0.16270094\nIteration 1601, loss = 0.16256741\nIteration 1602, loss = 0.16243405\nIteration 1603, loss = 0.16230091\nIteration 1604, loss = 0.16216799\nIteration 1605, loss = 0.16203511\nIteration 1606, loss = 0.16190251\nIteration 1607, loss = 0.16177006\nIteration 1608, loss = 0.16163773\nIteration 1609, loss = 0.16150565\nIteration 1610, loss = 0.16137365\nIteration 1611, loss = 0.16124191\nIteration 1612, loss = 0.16111039\nIteration 1613, loss = 0.16097898\nIteration 1614, loss = 0.16084778\nIteration 1615, loss = 0.16071685\nIteration 1616, loss = 0.16058609\nIteration 1617, loss = 0.16045541\nIteration 1618, loss = 0.16032500\nIteration 1619, loss = 0.16019468\nIteration 1620, loss = 0.16006461\nIteration 1621, loss = 0.15993467\nIteration 1622, loss = 0.15980491\nIteration 1623, loss = 0.15967538\nIteration 1624, loss = 0.15954599\nIteration 1625, loss = 0.15941673\nIteration 1626, loss = 0.15928772\nIteration 1627, loss = 0.15915887\nIteration 1628, loss = 0.15903017\nIteration 1629, loss = 0.15890168\nIteration 1630, loss = 0.15877329\nIteration 1631, loss = 0.15864522\nIteration 1632, loss = 0.15851717\nIteration 1633, loss = 0.15838937\nIteration 1634, loss = 0.15826171\nIteration 1635, loss = 0.15813431\nIteration 1636, loss = 0.15800698\nIteration 1637, loss = 0.15787991\nIteration 1638, loss = 0.15775292\nIteration 1639, loss = 0.15762614\nIteration 1640, loss = 0.15749954\nIteration 1641, loss = 0.15737301\nIteration 1642, loss = 0.15724679\nIteration 1643, loss = 0.15712061\nIteration 1644, loss = 0.15699466\nIteration 1645, loss = 0.15686883\nIteration 1646, loss = 0.15674324\nIteration 1647, loss = 0.15661778\nIteration 1648, loss = 0.15649245\nIteration 1649, loss = 0.15636730\nIteration 1650, loss = 0.15624237\nIteration 1651, loss = 0.15611755\nIteration 1652, loss = 0.15599289\nIteration 1653, loss = 0.15586836\nIteration 1654, loss = 0.15574414\nIteration 1655, loss = 0.15561991\nIteration 1656, loss = 0.15549598\nIteration 1657, loss = 0.15537212\nIteration 1658, loss = 0.15524845\nIteration 1659, loss = 0.15512500\nIteration 1660, loss = 0.15500162\nIteration 1661, loss = 0.15487847\nIteration 1662, loss = 0.15475549\nIteration 1663, loss = 0.15463261\nIteration 1664, loss = 0.15450985\nIteration 1665, loss = 0.15438735\nIteration 1666, loss = 0.15426488\nIteration 1667, loss = 0.15414257\nIteration 1668, loss = 0.15402042\nIteration 1669, loss = 0.15389845\nIteration 1670, loss = 0.15377664\nIteration 1671, loss = 0.15365499\nIteration 1672, loss = 0.15353341\nIteration 1673, loss = 0.15341212\nIteration 1674, loss = 0.15329086\nIteration 1675, loss = 0.15316983\nIteration 1676, loss = 0.15304891\nIteration 1677, loss = 0.15292823\nIteration 1678, loss = 0.15280761\nIteration 1679, loss = 0.15268725\nIteration 1680, loss = 0.15256697\nIteration 1681, loss = 0.15244685\nIteration 1682, loss = 0.15232694\nIteration 1683, loss = 0.15220710\nIteration 1684, loss = 0.15208750\nIteration 1685, loss = 0.15196803\nIteration 1686, loss = 0.15184871\nIteration 1687, loss = 0.15172950\nIteration 1688, loss = 0.15161062\nIteration 1689, loss = 0.15149179\nIteration 1690, loss = 0.15137329\nIteration 1691, loss = 0.15125483\nIteration 1692, loss = 0.15113656\nIteration 1693, loss = 0.15101844\nIteration 1694, loss = 0.15090049\nIteration 1695, loss = 0.15078268\nIteration 1696, loss = 0.15066508\nIteration 1697, loss = 0.15054756\nIteration 1698, loss = 0.15043022\nIteration 1699, loss = 0.15031303\nIteration 1700, loss = 0.15019603\nIteration 1701, loss = 0.15007910\nIteration 1702, loss = 0.14996242\nIteration 1703, loss = 0.14984585\nIteration 1704, loss = 0.14972943\nIteration 1705, loss = 0.14961312\nIteration 1706, loss = 0.14949708\nIteration 1707, loss = 0.14938111\nIteration 1708, loss = 0.14926533\nIteration 1709, loss = 0.14914964\nIteration 1710, loss = 0.14903411\nIteration 1711, loss = 0.14891878\nIteration 1712, loss = 0.14880357\nIteration 1713, loss = 0.14868849\nIteration 1714, loss = 0.14857360\nIteration 1715, loss = 0.14845881\nIteration 1716, loss = 0.14834423\nIteration 1717, loss = 0.14822970\nIteration 1718, loss = 0.14811548\nIteration 1719, loss = 0.14800122\nIteration 1720, loss = 0.14788721\nIteration 1721, loss = 0.14777336\nIteration 1722, loss = 0.14765959\nIteration 1723, loss = 0.14754599\nIteration 1724, loss = 0.14743254\nIteration 1725, loss = 0.14731930\nIteration 1726, loss = 0.14720616\nIteration 1727, loss = 0.14709318\nIteration 1728, loss = 0.14698033\nIteration 1729, loss = 0.14686766\nIteration 1730, loss = 0.14675513\nIteration 1731, loss = 0.14664274\nIteration 1732, loss = 0.14653046\nIteration 1733, loss = 0.14641844\nIteration 1734, loss = 0.14630639\nIteration 1735, loss = 0.14619463\nIteration 1736, loss = 0.14608296\nIteration 1737, loss = 0.14597138\nIteration 1738, loss = 0.14585997\nIteration 1739, loss = 0.14574868\nIteration 1740, loss = 0.14563756\nIteration 1741, loss = 0.14552657\nIteration 1742, loss = 0.14541565\nIteration 1743, loss = 0.14530496\nIteration 1744, loss = 0.14519433\nIteration 1745, loss = 0.14508392\nIteration 1746, loss = 0.14497357\nIteration 1747, loss = 0.14486336\nIteration 1748, loss = 0.14475336\nIteration 1749, loss = 0.14464343\nIteration 1750, loss = 0.14453365\nIteration 1751, loss = 0.14442407\nIteration 1752, loss = 0.14431454\nIteration 1753, loss = 0.14420522\nIteration 1754, loss = 0.14409592\nIteration 1755, loss = 0.14398696\nIteration 1756, loss = 0.14387797\nIteration 1757, loss = 0.14376922\nIteration 1758, loss = 0.14366059\nIteration 1759, loss = 0.14355201\nIteration 1760, loss = 0.14344367\nIteration 1761, loss = 0.14333539\nIteration 1762, loss = 0.14322729\nIteration 1763, loss = 0.14311929\nIteration 1764, loss = 0.14301144\nIteration 1765, loss = 0.14290372\nIteration 1766, loss = 0.14279616\nIteration 1767, loss = 0.14268873\nIteration 1768, loss = 0.14258144\nIteration 1769, loss = 0.14247433\nIteration 1770, loss = 0.14236731\nIteration 1771, loss = 0.14226043\nIteration 1772, loss = 0.14215368\nIteration 1773, loss = 0.14204708\nIteration 1774, loss = 0.14194066\nIteration 1775, loss = 0.14183426\nIteration 1776, loss = 0.14172813\nIteration 1777, loss = 0.14162204\nIteration 1778, loss = 0.14151606\nIteration 1779, loss = 0.14141028\nIteration 1780, loss = 0.14130464\nIteration 1781, loss = 0.14119908\nIteration 1782, loss = 0.14109369\nIteration 1783, loss = 0.14098836\nIteration 1784, loss = 0.14088327\nIteration 1785, loss = 0.14077821\nIteration 1786, loss = 0.14067332\nIteration 1787, loss = 0.14056854\nIteration 1788, loss = 0.14046396\nIteration 1789, loss = 0.14035940\nIteration 1790, loss = 0.14025505\nIteration 1791, loss = 0.14015080\nIteration 1792, loss = 0.14004670\nIteration 1793, loss = 0.13994262\nIteration 1794, loss = 0.13983875\nIteration 1795, loss = 0.13973499\nIteration 1796, loss = 0.13963132\nIteration 1797, loss = 0.13952780\nIteration 1798, loss = 0.13942451\nIteration 1799, loss = 0.13932124\nIteration 1800, loss = 0.13921819\nIteration 1801, loss = 0.13911522\nIteration 1802, loss = 0.13901238\nIteration 1803, loss = 0.13890967\nIteration 1804, loss = 0.13880708\nIteration 1805, loss = 0.13870465\nIteration 1806, loss = 0.13860230\nIteration 1807, loss = 0.13850010\nIteration 1808, loss = 0.13839796\nIteration 1809, loss = 0.13829604\nIteration 1810, loss = 0.13819416\nIteration 1811, loss = 0.13809246\nIteration 1812, loss = 0.13799079\nIteration 1813, loss = 0.13788942\nIteration 1814, loss = 0.13778793\nIteration 1815, loss = 0.13768673\nIteration 1816, loss = 0.13758559\nIteration 1817, loss = 0.13748456\nIteration 1818, loss = 0.13738367\nIteration 1819, loss = 0.13728290\nIteration 1820, loss = 0.13718229\nIteration 1821, loss = 0.13708173\nIteration 1822, loss = 0.13698129\nIteration 1823, loss = 0.13688106\nIteration 1824, loss = 0.13678078\nIteration 1825, loss = 0.13668069\nIteration 1826, loss = 0.13658067\nIteration 1827, loss = 0.13648090\nIteration 1828, loss = 0.13638108\nIteration 1829, loss = 0.13628150\nIteration 1830, loss = 0.13618193\nIteration 1831, loss = 0.13608257\nIteration 1832, loss = 0.13598324\nIteration 1833, loss = 0.13588408\nIteration 1834, loss = 0.13578506\nIteration 1835, loss = 0.13568610\nIteration 1836, loss = 0.13558731\nIteration 1837, loss = 0.13548858\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"}],"source":["from sklearn.neural_network import MLPClassifier\n","model = MLPClassifier(hidden_layer_sizes=(int(13**2+13), int((13**2+13)/2)),\n","                      activation=\"relu\", max_iter=5000, alpha=0.01, verbose=True, solver=\"sgd\")\n","h = model.fit(x_train, y_train)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Test"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["y_pred = model.predict(x_test)\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        19\n           1       1.00      0.95      0.98        21\n           2       1.00      1.00      1.00        14\n\n   micro avg       1.00      0.98      0.99        54\n   macro avg       1.00      0.98      0.99        54\nweighted avg       1.00      0.98      0.99        54\n samples avg       0.98      0.98      0.98        54\n\n[[19  0  0]\n [ 1 20  0]\n [ 0  0 14]]\nAccuracy is : 0.9814814814814815\n"},{"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x1b74427e6d0>]"},"execution_count":10,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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\n","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 378.465625 248.518125 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m5726768c7c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m5726768c7c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(48.502557 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"93.127657\" xlink:href=\"#m5726768c7c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 250 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(83.583907 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"134.571507\" xlink:href=\"#m5726768c7c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 500 -->\r\n      <g transform=\"translate(125.027757 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"176.015358\" xlink:href=\"#m5726768c7c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 750 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(166.471608 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"217.459208\" xlink:href=\"#m5726768c7c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 1000 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(204.734208 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"258.903058\" xlink:href=\"#m5726768c7c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1250 -->\r\n      <g transform=\"translate(246.178058 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"300.346908\" xlink:href=\"#m5726768c7c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 1500 -->\r\n      <g transform=\"translate(287.621908 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"341.790759\" xlink:href=\"#m5726768c7c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1750 -->\r\n      <g transform=\"translate(329.065759 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_9\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma6df48fcf6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma6df48fcf6\" y=\"202.719113\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.25 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 206.518332)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma6df48fcf6\" y=\"176.439528\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.50 -->\r\n      <g transform=\"translate(7.2 180.238746)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma6df48fcf6\" y=\"150.159942\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.75 -->\r\n      <g transform=\"translate(7.2 153.959161)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma6df48fcf6\" y=\"123.880357\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 1.00 -->\r\n      <g transform=\"translate(7.2 127.679575)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma6df48fcf6\" y=\"97.600771\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.25 -->\r\n      <g transform=\"translate(7.2 101.39999)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma6df48fcf6\" y=\"71.321186\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 1.50 -->\r\n      <g transform=\"translate(7.2 75.120404)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma6df48fcf6\" y=\"45.0416\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 1.75 -->\r\n      <g transform=\"translate(7.2 48.840819)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma6df48fcf6\" y=\"18.762015\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 2.00 -->\r\n      <g transform=\"translate(7.2 22.561233)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p8dfaa38c32)\" d=\"M 51.683807 17.083636 \r\nL 52.015358 17.38651 \r\nL 52.512684 18.178589 \r\nL 53.175785 19.676746 \r\nL 54.501989 23.405634 \r\nL 56.325518 28.413064 \r\nL 58.149047 32.750457 \r\nL 59.641026 35.693104 \r\nL 61.133005 38.170305 \r\nL 62.956534 40.75301 \r\nL 65.608941 44.124149 \r\nL 69.421775 48.568299 \r\nL 77.047443 57.351502 \r\nL 81.523379 62.88529 \r\nL 85.999315 68.798674 \r\nL 90.309475 74.861644 \r\nL 95.448513 82.458993 \r\nL 100.090224 89.704126 \r\nL 110.69985 106.872228 \r\nL 119.651721 121.152897 \r\nL 125.288085 129.721692 \r\nL 130.261347 136.883193 \r\nL 134.737283 142.948715 \r\nL 138.881668 148.218843 \r\nL 143.026053 153.127148 \r\nL 147.004662 157.497558 \r\nL 150.983272 161.540188 \r\nL 154.961882 165.269438 \r\nL 159.106267 168.842642 \r\nL 163.250652 172.127268 \r\nL 167.560812 175.254944 \r\nL 171.870973 178.109637 \r\nL 176.346908 180.812635 \r\nL 180.98862 183.361319 \r\nL 185.961882 185.835623 \r\nL 191.100919 188.144752 \r\nL 196.571507 190.359394 \r\nL 202.539422 192.532361 \r\nL 208.838887 194.586611 \r\nL 215.801454 196.613678 \r\nL 223.261347 198.548234 \r\nL 231.218566 200.379015 \r\nL 239.838887 202.132709 \r\nL 249.45386 203.862313 \r\nL 260.063486 205.543621 \r\nL 271.833539 207.181431 \r\nL 284.929796 208.774717 \r\nL 299.518031 210.318876 \r\nL 315.929796 211.825375 \r\nL 334.330866 213.283813 \r\nL 354.72124 214.673013 \r\nL 356.047443 214.756364 \r\nL 356.047443 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p8dfaa38c32\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data","transient":{}}],"source":["from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, accuracy_score\n","\n","print(classification_report(y_test,y_pred))\n","print(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1)))\n","print(f\"Accuracy is : {accuracy_score(y_pred,y_test)}\")\n","plt.plot(h.loss_curve_)\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}