{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d7acda",
   "metadata": {},
   "source": [
    "## MXNet Logistic Regression\n",
    "\n",
    "Example of multi class Logistic Regression implementation on MXNet with Embedding layer. Run on Python 3.10 with these libraries.\n",
    "\n",
    "```\n",
    "mxnet-cu112==1.9.1\n",
    "scikit-learn==1.1.2\n",
    "onnx==1.12.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d9431",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ebc4f38-af45-4ea6-8929-49a7b9cf2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, symbol\n",
    "from mxnet.gluon import nn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa2a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "EPOCH = 10\n",
    "\n",
    "mx.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51254980",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Use sklearn to get 20 newsgroups dataset and create dictionary (word/ID pair).\n",
    "* Since sklearn doesn't have function to turn word into ID, it's done manually.\n",
    "* Don't forget to pad or trim to match Logistic Regression input length.\n",
    "* Value of `len(vectorizer.vocabulary_)` is used as padding since `0` to `len(vectorizer.vocabulary_)-1` already used by dictionary. Another approach where `0` used as padding and increment dictionary ID by 1 also works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d97969-026d-438a-838b-3badb92d4311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "ng_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quote'))\n",
    "ng_test  = fetch_20newsgroups(subset='test',  remove=('headers', 'footers', 'quote'))\n",
    "\n",
    "# Use vectorizer to create dictionary\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.6, min_df=25)\n",
    "vectorizer.fit(ng_train.data)\n",
    "\n",
    "# Process text\n",
    "X_train = nd.empty((len(ng_train.data), MAX_LEN ), dtype='int')\n",
    "X_test  = nd.empty((len(ng_test.data),  MAX_LEN ), dtype='int')\n",
    "for idx, X in enumerate(ng_train.data + ng_test.data):\n",
    "    # Split words using vectorizer regex and convert word to word ID (e.g. \"amd\" to 529)\n",
    "    words = re.findall(vectorizer.token_pattern, X)\n",
    "    words_id = [vectorizer.vocabulary_[w] for w in words if w in vectorizer.vocabulary_]\n",
    "    \n",
    "    # Padding or remove left over\n",
    "    if len(words_id) > MAX_LEN:\n",
    "        words_id = words_id[:MAX_LEN]\n",
    "    else:\n",
    "        words_id.extend([len(vectorizer.vocabulary_)] * (MAX_LEN - len(words_id)) )\n",
    "\n",
    "    words_id = nd.array(words_id)\n",
    "    if idx < len(ng_train.data):\n",
    "        X_train[idx] = words_id\n",
    "    else:\n",
    "        X_test[idx-len(ng_train.data)] = words_id\n",
    "\n",
    "\n",
    "# Process label\n",
    "y_train, y_test = nd.array(ng_train.target), nd.array(ng_test.target)\n",
    "label = ng_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a4bb45",
   "metadata": {},
   "source": [
    "### Prepare train\n",
    "\n",
    "* Use hybrid model to improve training/inference speed.\n",
    "* \"Vanilla\" logistic regresion use Sigmoid and L2 loss, but this example use Softmax and Softmax Cross Entropy Loss.\n",
    "* `gluon.loss.SoftmaxCrossEntropyLoss` could handle integer/one-hot-encode as true label and logit/softmax value as predicted label.\n",
    "* Freeze `count` embedding so MXNet will only focus to find weight of each word for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6b1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Net, self).__init__(**kwargs)\n",
    "        self.weight = nn.Embedding(\n",
    "            len(vectorizer.vocabulary_), 20,\n",
    "            weight_initializer=mx.initializer.Xavier()\n",
    "        )\n",
    "        self.count  = nn.Embedding(\n",
    "            len(vectorizer.vocabulary_), 1,\n",
    "            weight_initializer=mx.initializer.One()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        \n",
    "    def hybrid_forward(self, F, x):\n",
    "        weight = self.weight(x)\n",
    "        count  = self.count(x)\n",
    "        # x = nd.linalg_gemm2(weight, count, transpose_a=True)\n",
    "        x = symbol.linalg_gemm2(weight, count, transpose_a=True)\n",
    "        x = self.flatten(x)\n",
    "        # x = nd.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c8982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.dataset.ArrayDataset(X_train, y_train),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_data  = gluon.data.DataLoader(\n",
    "    gluon.data.dataset.ArrayDataset(X_test,  y_test),\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "net = Net()\n",
    "net.initialize()\n",
    "net.hybridize()\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "# embedding0_weight means only train self.weight on Net()\n",
    "trainer = gluon.Trainer(net.collect_params('embedding0_weight'), 'adam', {'learning_rate': LR})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ecdf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(output, label):\n",
    "    # output: (batch, num_output) float32 ndarray\n",
    "    # label: (batch, ) int32 ndarray\n",
    "    return (output.argmax(axis=1) ==\n",
    "            label.astype('float32')).mean().asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062430c",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f726b8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 2.142, train acc 0.509, test acc 0.563\n",
      "Epoch 1: loss 1.229, train acc 0.788, test acc 0.628\n",
      "Epoch 2: loss 0.927, train acc 0.844, test acc 0.644\n",
      "Epoch 3: loss 0.752, train acc 0.882, test acc 0.645\n",
      "Epoch 4: loss 0.635, train acc 0.901, test acc 0.646\n",
      "Epoch 5: loss 0.552, train acc 0.914, test acc 0.664\n",
      "Epoch 6: loss 0.483, train acc 0.926, test acc 0.664\n",
      "Epoch 7: loss 0.432, train acc 0.933, test acc 0.676\n",
      "Epoch 8: loss 0.387, train acc 0.941, test acc 0.667\n",
      "Epoch 9: loss 0.350, train acc 0.947, test acc 0.667\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    train_loss, train_acc, valid_acc = 0., 0., 0.\n",
    "    for data, label in train_data:\n",
    "        # forward + backward\n",
    "        with mx.autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        trainer.step(BATCH_SIZE)\n",
    "\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean().asscalar()\n",
    "        train_acc += acc(output, label)\n",
    "\n",
    "    # calculate test accuracy, then show training progress\n",
    "    for data, label in test_data:\n",
    "        valid_acc += acc(net(data), label)\n",
    "    print(\"Epoch %d: loss %.3f, train acc %.3f, test acc %.3f\" % (\n",
    "            epoch, train_loss/len(train_data), train_acc/len(train_data),\n",
    "            valid_acc/len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b674d3",
   "metadata": {},
   "source": [
    "### Export to ONNX\n",
    "\n",
    "* `net.export` only works when hybrid model is used.\n",
    "* `net.save` use work both Gluon/imperative and Symbol/symbolic based model.\n",
    "* Export model to ONNX currently only works with hybrid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b5e830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logreg-model.onnx'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.export('logreg')\n",
    "\n",
    "sym = './logreg-symbol.json'\n",
    "params = f'./logreg-0000.params'\n",
    "in_shapes = [(1, MAX_LEN)]\n",
    "in_types = ['int']\n",
    "onnx_file_path = './logreg-model.onnx'\n",
    "\n",
    "mx.onnx.export_model(sym, params, in_shapes, in_types, onnx_file_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8222a472c8547bbaeb3a9fb5c61377b1d9ba1a7ac73b7f9e51b9137ef6c720de"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
